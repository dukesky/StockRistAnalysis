{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb576d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "# from prophet import Prophet\n",
    "\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_forecasting.data.examples import get_stallion_data\n",
    "# data = get_stallion_data()  # load data as pandas dataframe\n",
    "\n",
    "vent = pd.read_csv('data/example/train_OwBvO8W/event_calendar.csv')\n",
    "historical = pd.read_csv('data/example/train_OwBvO8W/historical_volume.csv')\n",
    "soda = pd.read_csv('data/example/train_OwBvO8W/industry_soda_sales.csv')\n",
    "industry = pd.read_csv('data/example/train_OwBvO8W/industry_volume.csv')\n",
    "price = pd.read_csv('data/example/train_OwBvO8W/price_sales_promotion.csv')\n",
    "weather = pd.read_csv('data/example/train_OwBvO8W/weather.csv')\n",
    "\n",
    "data = historical.merge(price,on=['Agency','SKU','YearMonth'],how='left')\n",
    "data = data.merge(soda,on=['YearMonth'],how='left')\n",
    "data = data.merge(industry,on='YearMonth',how='left')\n",
    "data = data.merge(event,on=['YearMonth'],how='left')\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.iloc[[291,871,19532],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [column_name.lower() for column_name in data.columns]\n",
    "def create_date_column(x):\n",
    "    if not x.yearmonth :\n",
    "        return None\n",
    "    else:\n",
    "        return pd.datetime(year=int(x.yearmonth)//100,month=int(x.yearmonth)%100,day=1)\n",
    "data.loc[:,'date'] = data.apply(lambda x:create_date_column(x), axis=1)\n",
    "data.columns = [column_name.lower() for column_name in data.columns]\n",
    "data = data.rename(columns={\n",
    "                            'price':'price_regular','sales':'price_actual',\n",
    "                            'promotions':'discount'\n",
    "                           })\n",
    "def calculate_discount_percentage(x):\n",
    "    if x.price_regular == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x.discount/ x.price_regular\n",
    "data.loc[:,'discount_in_percent'] = data.apply(lambda x:calculate_discount_percentage(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "# add additional features\n",
    "# categories have to be strings\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = (\n",
    "    data\n",
    "    .groupby([\"time_idx\", \"sku\"], observed=True)\n",
    "    .volume.transform(\"mean\")\n",
    ")\n",
    "data[\"avg_volume_by_agency\"] = (\n",
    "    data\n",
    "    .groupby([\"time_idx\", \"agency\"], observed=True)\n",
    "    .volume.transform(\"mean\")\n",
    ")\n",
    "# we want to encode special days as one variable and \n",
    "# thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter day\", \"good friday\", \"new year\", \"christmas\",\n",
    "    \"labor day\", \"independence day\", \"revolution day memorial\",\n",
    "    \"regional games \", \"fifa u-17 world cup\", \"football gold cup\",\n",
    "    \"beer capital\", \"music fest\"\n",
    "]\n",
    "data[special_days] = (\n",
    "    data[special_days]\n",
    "    .apply(lambda x: x.map({0: \"-\", 1: x.name}))\n",
    "    .astype(\"category\")\n",
    ")\n",
    "# show sample data\n",
    "data.sample(10, random_state=521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import (\n",
    "    TimeSeriesDataSet,\n",
    "    GroupNormalizer\n",
    ")\n",
    "max_prediction_length = 6  # forecast 6 months\n",
    "max_encoder_length = 24  # use 24 months of history\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=0,  # allow predictions without history\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\n",
    "#         \"avg_population_2017\",\n",
    "#         \"avg_yearly_household_income_2017\"\n",
    "    ],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    # group of categorical variables can be treated as \n",
    "    # one variable\n",
    "    variable_groups={\"special_days\": special_days},\n",
    "    time_varying_known_reals=[\n",
    "        \"time_idx\",\n",
    "        \"price_regular\",\n",
    "        \"discount_in_percent\"\n",
    "    ],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "#         \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"]\n",
    "    ),  # use softplus with beta=1.0 and normalize by group\n",
    "    add_relative_time_idx=True,  # add as feature\n",
    "    add_target_scales=True,  # add as feature\n",
    "    add_encoder_length=True,  # add as feature\n",
    ")\n",
    "# create validation set (predict=True) which means to predict the\n",
    "# last max_prediction_length points in time for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, data, predict=True, stop_randomization=True\n",
    ")\n",
    "# create dataloaders for model\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 10, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da9d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "# stop training, when loss metric does not improve on validation set\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "# lr_logger = LearningRateLogger()  # log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # log to tensorboard\n",
    "# create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,  # train on CPU, use gpus = [0] to run on GPU\n",
    "    gradient_clip_val=0.1,\n",
    "#     early_stop_callback=early_stop_callback,\n",
    "    limit_train_batches=30,  # running validation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to quickly check for bugs\n",
    "    callbacks=[lr_logger],\n",
    "    logger=logger,\n",
    ")\n",
    "# initialise model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # biggest influence network size\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # QuantileLoss has 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # log example every 10 batches\n",
    "    reduce_on_plateau_patience=4,  # reduce learning automatically\n",
    ")\n",
    "tft.size() # 29.6k parameters in model\n",
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb318ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MAE,MAPE\n",
    "# load the best model according to the validation loss (given that\n",
    "# we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# calculate mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in val_dataloader])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "previous = torch.cat([x['encoder_target'] for x, y in val_dataloader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f136148",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [11,12,13]\n",
    "for i in index:\n",
    "    plt.plot(previous[i],label='previous')\n",
    "    plt.plot([24+i for i in range(len(predictions[i]))],predictions[i],label='predict')\n",
    "    plt.plot([24+i for i in range(len(actuals[i]))], actuals[i],label='true')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(true,pred, basic_reduction = 10, return_mean=False):\n",
    "    mape = []\n",
    "    for i in range(0,len(true)):\n",
    "        pred_val, true_val = pred[i], true[i]\n",
    "        if pred_val < 0:\n",
    "            pred_val = 0\n",
    "        if  true_val < basic_reduction:\n",
    "            true_val = basic_reduction\n",
    "        mape.append(abs(pred_val- true_val)/true_val)\n",
    "    if return_mean:\n",
    "        return sum(mape)/len(mape)\n",
    "    else:\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import SMAPE\n",
    "# calculate metric by which to display\n",
    "predictions = best_tft.predict(val_dataloader )\n",
    "mean_losses = SMAPE(reduction=\"none\")(predictions, actuals).mean(1)\n",
    "indices = mean_losses.argsort(descending=True)  # sort losses\n",
    "raw_predictions,x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "# show only two examples for demonstration purposes\n",
    "for idx in range(5):\n",
    "    best_tft.plot_prediction(\n",
    "        x,\n",
    "        raw_predictions,\n",
    "        idx=indices[idx],\n",
    "#         add_loss_to_title=SMAPE()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc995fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation = best_tft.interpret_output(\n",
    "    raw_predictions, reduction=\"sum\"\n",
    ")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data.agency=='Agency_22') & (data.sku=='SKU_01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810bc86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99ddd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829adc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
